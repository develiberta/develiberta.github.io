<!DOCTYPE html><html lang="en" mode="light" ><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><meta name="day-prompt" content="days ago"><meta name="hour-prompt" content="hours ago"><meta name="minute-prompt" content="minutes ago"><meta name="justnow-prompt" content="just now"><meta name="generator" content="Jekyll v4.4.1" /><meta property="og:title" content="nvidia/deeops 라이브러리를 이용한 모니터링(prometheus, grafana) 테스트 구성" /><meta name="author" content="Develiberta" /><meta property="og:locale" content="en" /><meta name="description" content="목적 deepops를 이용해서 필요한 환경을 구성할 수 있다. deepops를 이용해서 GPU 모니터링을 위한 prometheus, grafana 환경을 구성할 수 있다." /><meta property="og:description" content="목적 deepops를 이용해서 필요한 환경을 구성할 수 있다. deepops를 이용해서 GPU 모니터링을 위한 prometheus, grafana 환경을 구성할 수 있다." /><link rel="canonical" href="https://develiberta.github.io/posts/deepops_test/" /><meta property="og:url" content="https://develiberta.github.io/posts/deepops_test/" /><meta property="og:site_name" content="Develiberta" /><meta property="og:type" content="article" /><meta property="article:published_time" content="2022-06-08T17:30:00+09:00" /><meta name="twitter:card" content="summary" /><meta property="twitter:title" content="nvidia/deeops 라이브러리를 이용한 모니터링(prometheus, grafana) 테스트 구성" /><meta name="twitter:site" content="@develiberta" /><meta name="twitter:creator" content="@Develiberta" /><meta name="google-site-verification" content="google_meta_tag_verification" /> <script type="application/ld+json"> {"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"Develiberta"},"dateModified":"2022-10-03T11:57:33+09:00","datePublished":"2022-06-08T17:30:00+09:00","description":"목적 deepops를 이용해서 필요한 환경을 구성할 수 있다. deepops를 이용해서 GPU 모니터링을 위한 prometheus, grafana 환경을 구성할 수 있다.","headline":"nvidia/deeops 라이브러리를 이용한 모니터링(prometheus, grafana) 테스트 구성","mainEntityOfPage":{"@type":"WebPage","@id":"https://develiberta.github.io/posts/deepops_test/"},"url":"https://develiberta.github.io/posts/deepops_test/"}</script><title>nvidia/deeops 라이브러리를 이용한 모니터링(prometheus, grafana) 테스트 구성 | Develiberta</title><link rel="apple-touch-icon" sizes="180x180" href="/assets/img/favicons/apple-touch-icon.png?v=1743601323"><link rel="icon" type="image/png" sizes="32x32" href="/assets/img/favicons/favicon-32x32.png?v=1743601323"><link rel="icon" type="image/png" sizes="16x16" href="/assets/img/favicons/favicon-16x16.png?v=1743601323"><link rel="manifest" href="/assets/img/favicons/site.webmanifest?v=1743601323"><link rel="shortcut icon" href="/assets/img/favicons/favicon.ico?v=1743601323"><meta name="apple-mobile-web-app-title" content="Develiberta"><meta name="application-name" content="Develiberta"><meta name="msapplication-TileColor" content="#da532c"><meta name="msapplication-config" content="/assets/img/favicons/browserconfig.xml"><meta name="theme-color" content="#ffffff"><link rel="preconnect" href="https://fonts.gstatic.com" crossorigin="anonymous"><link rel="dns-prefetch" href="https://fonts.gstatic.com"><link rel="preconnect" href="https://www.google-analytics.com" crossorigin="use-credentials"><link rel="dns-prefetch" href="https://www.google-analytics.com"><link rel="preconnect" href="https://www.googletagmanager.com" crossorigin="anonymous"><link rel="dns-prefetch" href="https://www.googletagmanager.com"><link rel="preconnect" href="https://cdn.jsdelivr.net"><link rel="dns-prefetch" href="https://cdn.jsdelivr.net"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.0.0/dist/css/bootstrap.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.11.2/css/all.min.css"><link rel="stylesheet" href="/assets/css/style.css?v=1743601323"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.css?v=1743601323"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/magnific-popup@1.1.0/dist/magnific-popup.min.css"> <script src="https://cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script><body data-spy="scroll" data-target="#toc"><div id="sidebar" class="d-flex flex-column align-items-end" lang="en"><div class="profile-wrapper text-center"><div id="avatar"> <a href="/?v=1743601323" alt="avatar" class="mx-auto"> <img src="/assets/img/favicons/android-chrome-512x512.png" alt="avatar" onerror="this.style.display='none'"> </a></div><div class="site-title mt-3"> <a href="/?v=1743601323">Develiberta</a></div><div class="site-subtitle font-italic"></div></div><ul class="w-100"><li class="nav-item"> <a href="/?v=1743601323" class="nav-link"> <i class="fa-fw fas fa-home ml-xl-3 mr-xl-3 unloaded"></i> <span>HOME</span> </a><li class="nav-item"> <a href="/categories/?v=1743601323" class="nav-link"> <i class="fa-fw fas fa-stream ml-xl-3 mr-xl-3 unloaded"></i> <span>CATEGORIES</span> </a><li class="nav-item"> <a href="/tags/?v=1743601323" class="nav-link"> <i class="fa-fw fas fa-tags ml-xl-3 mr-xl-3 unloaded"></i> <span>TAGS</span> </a><li class="nav-item"> <a href="/archives/?v=1743601323" class="nav-link"> <i class="fa-fw fas fa-archive ml-xl-3 mr-xl-3 unloaded"></i> <span>ARCHIVES</span> </a><li class="nav-item"> <a href="/about/?v=1743601323" class="nav-link"> <i class="fa-fw fas fa-info ml-xl-3 mr-xl-3 unloaded"></i> <span>ABOUT</span> </a></ul><div class="sidebar-bottom mt-auto d-flex flex-wrap justify-content-center"> <a href="https://github.com/develiberta?v=1743601323" aria-label="github" target="_blank" rel="noopener"> <i class="fab fa-github-alt"></i> </a> <a href="https://twitter.com/develiberta?v=1743601323" aria-label="twitter" target="_blank" rel="noopener"> <i class="fab fa-twitter"></i> </a> <a href=" javascript:location.href = 'mailto:' + ['develiberta','gmail.com'].join('@')?v=1743601323" aria-label="email" > <i class="fas fa-envelope"></i> </a> <a href="/feed.xml?v=1743601323" aria-label="rss" > <i class="fas fa-rss"></i> </a></div></div><div id="topbar-wrapper" class="row justify-content-center topbar-down"><div id="topbar" class="col-11 d-flex h-100 align-items-center justify-content-between"> <span id="breadcrumb"> <span> <a href="/?v=1743601323"> Home </a> </span> <span>nvidia/deeops 라이브러리를 이용한 모니터링(prometheus, grafana) 테스트 구성</span> </span> <i id="sidebar-trigger" class="fas fa-bars fa-fw"></i><div id="topbar-title"> Post</div><i id="search-trigger" class="fas fa-search fa-fw"></i> <span id="search-wrapper" class="align-items-center"> <i class="fas fa-search fa-fw"></i> <input class="form-control" id="search-input" type="search" aria-label="search" autocomplete="off" placeholder="Search..."> <i class="fa fa-times-circle fa-fw" id="search-cleaner"></i> </span> <span id="search-cancel" >Cancel</span></div></div><div id="main-wrapper"><div id="main"><div class="row"><div id="post-wrapper" class="col-12 col-lg-11 col-xl-8"><div class="post pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4"><h1 data-toc-skip>nvidia/deeops 라이브러리를 이용한 모니터링(prometheus, grafana) 테스트 구성</h1><div class="post-meta text-muted d-flex flex-column"><div> <span class="semi-bold"> Develiberta </span> on <span class="timeago " data-toggle="tooltip" data-placement="bottom" title="Wed, Jun 8, 2022, 5:30 PM +0900" >Jun 8, 2022<i class="unloaded">2022-06-08T17:30:00+09:00</i> </span></div><div> <span> Updated <span class="timeago lastmod" data-toggle="tooltip" data-placement="bottom" title="Mon, Oct 3, 2022, 11:57 AM +0900" >Oct 3, 2022<i class="unloaded">2022-10-03T11:57:33+09:00</i> </span> </span> <span class="readtime" data-toggle="tooltip" data-placement="bottom" title="3915 words">21 min read</span></div></div><div class="post-content"><h2 id="목적">목적</h2><hr /><ol><li>deepops를 이용해서 필요한 환경을 구성할 수 있다.<li>deepops를 이용해서 GPU 모니터링을 위한 prometheus, grafana 환경을 구성할 수 있다.</ol><h2 id="실천-목표">실천 목표</h2><hr /><ol><li>deepops를 이용해서 nvidia driver, docker, prometheus, grafana를 설치하고 모니터링한다.<li>deepops를 이용해서 (nvidia driver, docker 설치는 되어 있다고 가정하고 ★) prometheus, grafana를 설치하고 모니터링한다.</ol><h2 id="deepops-개요">deepops 개요</h2><hr /><ol><li>https://github.com/NVIDIA/deepops<li>GPU 인프라 및 자동화 도구</ol><h2 id="deepops를-이용한-테스트-환경-구성">deepops를 이용한 테스트 환경 구성</h2><hr /><ol><li>테스트 구성도 <img data-proofer-ignore data-src="/assets/img/illustrations/2022-06-02-deepops_test.png" alt="deepops_test" /> <em>Copyrightⓒ2022 Develiberta All rights reserved.</em><ul><li>테스트는 deepops 22.04.2 버전을 기준으로 작성되었으며, OS는 Ubuntu 20.04 LTS를 설치했다.<li>master 노드에 gpu가 있는 경우, provisioning 노드와 master 노드를 하나의 서버로 통합하지 않는다. (위의 그림처럼 하는 경우 별도의 작업 필요) 왜냐하면 provisioning 노드에서 각 노드(master 노드와 worker 노드)에 필요한 프로그램을 설치하는 과정에서 각 노드가 재부팅될 가능성이 있는데, 설치를 주관하는 provisioning 노드가 재부팅되면 설치가 비정상적으로 종료되기 때문이다.<li>파일 내에서 추가나 변경 또는 삭제가 필요한 부분은 ☆로 표시했다.</ul><li>모든 노드에 지원되는 OS를 설치한다. 지원되는 OS는 다음과 같다.<ul><li>NVIDIA DGX OS 4, 5<li>Ubuntu 18.04 LTS, 20.04 LTS<li>CentOS 7, 8</ul><li>provisioning 노드에서 기본적인 deepops를 설치한다.<div lang="console" class="language-console highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
</pre><td class="rouge-code"><pre><span class="gp"> $</span><span class="w"> </span>git clone https://github.com/NVIDIA/deepops.git
<span class="gp"> $</span><span class="w"> </span><span class="nb">cd </span>deepops
<span class="gp"> $</span><span class="w"> </span>./scripts/setup.sh
</pre></table></code></div></div><li>provisioning 노드에서 deepops inventory를 수정한다.<div lang="console" class="language-console highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre><td class="rouge-code"><pre><span class="gp"> $</span><span class="w"> </span>vi ./config/inventory
</pre></table></code></div></div><div lang="shell" class="language-shell highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
</pre><td class="rouge-code"><pre> <span class="o">[</span>all]
 <span class="c"># ☆ 다음을 추가 (이때, 각 서버의 hostname과 IP를 동일하게 추가)</span>
 deepops-mp	<span class="nv">ansible_host</span><span class="o">=</span>192.168.30.119
 deepops-w	<span class="nv">ansible_host</span><span class="o">=</span>192.168.30.121
	
 <span class="o">[</span>slurm-master]
 <span class="c"># ☆ 다음을 추가</span>
 deepops-mp	<span class="c"># master 노드</span>
	
 <span class="o">[</span>slurm-node]
 <span class="c"># ☆ 다음을 추가</span>
 deepops-mp	<span class="c"># master 노드에 대해서도 모니터링하고자 하는 경우에만 추가</span>
 deepops-w	<span class="c"># worker 노드</span>
	
 <span class="o">[</span>all:vars]
 <span class="c"># SSH User</span>
 <span class="c"># ☆ 다음을 추가</span>
 <span class="nv">ansible_user</span><span class="o">=</span>nvidia	<span class="c"># ansible을 이용할 사용자명</span>
</pre></table></code></div></div><li>provisioning 노드에서 deepops inventory가 정상적으로 동작하는지 확인한다.<div lang="console" class="language-console highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre><td class="rouge-code"><pre><span class="gp"> $</span><span class="w"> </span>ansible all <span class="nt">-m</span> raw <span class="nt">-a</span> <span class="s2">"hostname"</span> <span class="nt">-k</span>
</pre></table></code></div></div><li>provisioning 노드에서 deepops 전체 설정 정보를 수정한다.<div lang="console" class="language-console highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre><td class="rouge-code"><pre><span class="gp"> $</span><span class="w"> </span>vi ./config/group_vars/all.yml
</pre></table></code></div></div><div lang="shell" class="language-shell highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60
61
62
63
64
65
66
67
68
69
70
71
72
73
74
75
76
77
78
79
80
81
82
83
84
85
86
87
88
89
90
91
92
93
94
95
96
97
98
99
100
101
102
103
104
105
106
107
108
109
110
111
112
113
114
115
116
117
118
119
120
121
122
123
124
125
126
127
128
129
130
131
132
133
134
135
136
137
138
139
140
141
142
143
144
145
146
147
148
149
150
151
152
153
154
155
156
157
158
159
160
161
162
163
164
165
166
167
168
169
170
171
172
173
174
175
176
177
178
179
180
181
182
183
184
185
186
187
188
189
190
191
192
193
194
195
196
197
198
199
200
201
202
203
204
205
206
207
208
209
210
211
212
213
214
215
216
217
218
219
220
221
222
223
224
225
226
227
228
229
230
231
232
233
234
235
236
237
238
239
240
241
242
243
244
245
246
247
248
249
250
251
252
253
254
255
256
257
258
259
260
261
262
263
264
265
266
267
268
269
270
271
272
273
274
275
276
277
278
279
280
281
282
283
284
285
286
287
288
289
290
291
292
293
294
295
296
297
298
299
300
301
302
303
304
305
306
307
308
309
310
</pre><td class="rouge-code"><pre> <span class="c">################################################################################</span>
 <span class="c"># DeepOps Ansible Config                                                       #</span>
 <span class="c">################################################################################</span>
 <span class="c">#</span>
 <span class="c"># Configuration common to all hosts</span>
 <span class="c"># Define per-group or per-host values in the other configuration files</span>
 <span class="c">#</span>
	
 <span class="c">################################################################################</span>
 <span class="c"># PROXY</span>
 <span class="c">################################################################################</span>
 <span class="c"># Proxy Variables needed for scripts, ansible, etc.</span>
 <span class="c"># http_proxy: http://10.0.2.5:3128</span>
 <span class="c"># https_proxy: http://10.0.2.5:3128</span>
 <span class="c"># no_proxy: localhost,cluster.local,127.0.0.1,::1,10.0.2.10,10.0.2.20,10.0.2.30</span>
	
 <span class="c"># proxy_env:</span>
     <span class="c"># http_proxy: '{{ http_proxy }}'</span>
     <span class="c"># https_proxy: '{{ https_proxy }}'</span>
     <span class="c"># no_proxy: '{{ no_proxy }}'</span>
	
	
 <span class="c">################################################################################</span>
 <span class="c"># NETWORK                                                                      #</span>
 <span class="c">################################################################################</span>
 <span class="c"># DNS config</span>
 <span class="c"># Playbook: dns-config</span>
 <span class="c">#dns_config_servers: [8.8.8.8]</span>
 <span class="c">#dns_config_search: [example.com]</span>
	
 <span class="c"># NTP configuration</span>
 <span class="c"># Playbook: chrony-client</span>
 chrony_install: <span class="nb">true
 </span>chrony_service_state: <span class="s2">"started"</span>
 chrony_service_enabled: <span class="s2">"yes"</span>
 chrony_timezone: <span class="s2">"Etc/UTC"</span>
 chrony_config_server:
 - 0.pool.ntp.org
 - 1.pool.ntp.org
 - 2.pool.ntp.org
 - 3.pool.ntp.org
	
 <span class="c"># Set hostname based on inventory file</span>
 <span class="c"># ☆ 다음을 변경 : 서버의 호스트명을 임의로 변경하지 않고, ansible에서의 호스트명을 서버의 호스트명으로 수동으로 맞추도록 함 (inventory 파일에서 수행)</span>
 deepops_set_hostname: <span class="nb">false</span>
	
 <span class="c">################################################################################</span>
 <span class="c"># SOFTWARE                                                                     #</span>
 <span class="c">################################################################################</span>
 <span class="c"># Extra software to install or remove</span>
 <span class="c"># Playbook: software</span>
 <span class="c">#software_extra_packages:</span>
 <span class="c">#  - curl</span>
 <span class="c">#  - git</span>
 <span class="c">#  - rsync</span>
 <span class="c">#  - screen</span>
 <span class="c">#  - tmux</span>
 <span class="c">#  - vim</span>
 <span class="c">#  - wget</span>
 <span class="c">#  - build-essential</span>
 <span class="c">#  - linux-tools-generic</span>
 <span class="c">#  - "{{ 'linux-headers-' + ansible_kernel }}"</span>
 <span class="c">#software_remove_packages:</span>
 <span class="c">#  - popularity-contest</span>
	
	
 <span class="c">################################################################################</span>
 <span class="c"># STORAGE                                                                      #</span>
 <span class="c">################################################################################</span>
 <span class="c"># AutoFS configuration</span>
 <span class="c"># Playbook: authentication</span>
 <span class="c">#autofs_mount: "/home"</span>
 <span class="c">#autofs_map: "auto.home_linux"</span>
	
 <span class="c"># NFS Server</span>
 <span class="c"># This config will create an NFS server and share the given exports</span>
 <span class="c"># Playbook: nfs-server.yml</span>
 <span class="c">#nfs_exports:</span>
 <span class="c">#  - path: /export/shared</span>
 <span class="c">#    options: "*(rw,sync,no_root_squash)"</span>
	
 <span class="c"># NFS Client</span>
 <span class="c"># This config will mount an NFS share on hosts</span>
 <span class="c"># Playbook: nfs-client.yml</span>
 <span class="c">#nfs_mounts:</span>
 <span class="c">#  - mountpoint: /mnt/shared</span>
 <span class="c">#    server: '{{ groups["slurm-master"][0] }}'</span>
 <span class="c">#    path: /export/shared</span>
 <span class="c">#    options: async,vers=3</span>
	
 <span class="c"># Extra Packages for Enterprise Linux (RHEL/CentOS)</span>
 epel_package: <span class="s2">"https://dl.fedoraproject.org/pub/epel/epel-release-latest-{{ ansible_distribution_major_version }}.noarch.rpm"</span>
	
 <span class="c">################################################################################</span>
 <span class="c"># USERS                                                                        #</span>
 <span class="c">################################################################################</span>
 <span class="c"># User management</span>
 <span class="c"># Playbook: users, user-password</span>
 <span class="c"># create user `nvidia` with password `deepops` on all nodes</span>
 <span class="nb">users</span>:
 - username: nvidia
     name: nvidia
     password: <span class="nv">$6$IrxI27V4ogJFfgTV$RvNskQFvXZzE9AFsIokuXKwDAyqs9Jd03Trfi7DsHoCyllK79</span>/zhAciZPENt4.2uRNMR0gE6.mRD/o9jP7WcZ.
     <span class="nb">groups</span>: <span class="o">[</span><span class="s1">'sudo'</span><span class="o">]</span>
     uid: 10007
     home: /home/nvidia
	
	
 <span class="c">################################################################################</span>
 <span class="c"># SSH HARDENING                                                                #</span>
 <span class="c">#   dev-sec.ssh-hardening role called from users playbook                      #</span>
 <span class="c">################################################################################</span>
	
 ssh_client_hardening: <span class="nb">false
 </span>ssh_server_password_login: <span class="nb">true
 </span>ssh_use_pam: <span class="nb">true
 </span>ssh_max_auth_retries: 10
 sftp_enabled: <span class="nb">true
 </span>sftp_chroot: <span class="nb">false</span>
	
 <span class="c">################################################################################</span>
 <span class="c"># NVIDIA                                                                       #</span>
 <span class="c">################################################################################</span>
 <span class="c"># NVIDIA GPU configuration</span>
 <span class="c"># Playbook: nvidia-cuda</span>
 cuda_version: cuda-toolkit-11-5
	
 <span class="c"># DGX-specific vars may be used to target specific models,</span>
 <span class="c"># because available versions for DGX may differ from the generic repo</span>
 <span class="c">#cuda_dgx_1_version: "{{ cuda_version }}"</span>
 <span class="c">#cuda_dgx_2_version: "{{ cuda_version }}"</span>
 <span class="c">#cuda_dgx_a100_version: "{{ cuda_version }}"</span>
	
 <span class="c"># Playbook: nvidia-set-gpu-clocks</span>
 <span class="c"># Resets the Gpu clocks to the default values. (see the `--reset-gpu-clocks` flag in nvidia-smi for more)</span>
 gpu_clock_reset: no
 <span class="c"># Specifies &lt;minGpuClock,maxGpuClock&gt; clocks as a pair (e.g. 1500,1500) that defines the range of desired locked GPU clock speed in MHz. Setting this will supersede application clocks and take effect regardless if an app is running. Input can also be a singular desired clock value. (see the `--lock-gpu-clocks` flag in nvidia-smi for more)</span>
 gpu_clock_lock: <span class="s2">"1507,1507"</span>
	
 <span class="c"># Debugging var: force install NVIDIA driver even if GPU not detected</span>
 nvidia_driver_force_install: <span class="nb">false</span>
	
	
 <span class="c">################################################################################</span>
 <span class="c"># CONTAINER RUNTIME                                                            #</span>
 <span class="c">################################################################################</span>
 <span class="c"># Docker configuration</span>
 <span class="c"># Playbook: docker, nvidia-docker, k8s-cluster</span>
 <span class="c">#</span>
 <span class="c"># For supported Docker versions, see: kubespray/roles/container-engine/docker/vars/*</span>
 docker_install: <span class="nb">yes
 </span>docker_version: <span class="s1">'20.10'</span>
 docker_dns_servers_strict: no
 docker_storage_options: <span class="nt">-s</span> overlay2
 <span class="c">#docker_options: "--bip=192.168.99.1/24"</span>
 <span class="c"># Enable docker iptables</span>
 <span class="c"># If this isn't set, containers won't have access to the outside net</span>
 <span class="c"># See https://github.com/kubernetes-sigs/kubespray/issues/2002</span>
 docker_iptables_enabled: <span class="nb">true</span>
 <span class="c"># Docker daemon logins</span>
 <span class="c">#   Note: example only! you should put these in an Ansible Vault file for security!</span>
 <span class="c">#</span>
 <span class="c">#docker_login_registries:</span>
 <span class="c">#- registry: docker.io</span>
 <span class="c">#  username: myuser</span>
 <span class="c">#  password: mypassword</span>
 <span class="c">#  email: docker@docker.io</span>
 <span class="c">#- registry: nvcr.io</span>
 <span class="c">#  username: '$oauthtoken'</span>
 <span class="c">#  password: mypassword</span>
	
 <span class="c"># Enroot configuration</span>
 <span class="c"># Playbook: slurm, slurm-cluster</span>
 <span class="c">#</span>
 <span class="c"># See: https://github.com/NVIDIA/pyxis/wiki/Setup#enroot-configuration-example</span>
 enroot_runtime_path: <span class="s1">'/run/enroot/user-$(id -u)'</span>
 enroot_cache_path: <span class="s1">'/var/lib/enroot-cache/user-$(id -u)'</span>
 enroot_data_path: <span class="s1">'/tmp/enroot-data/user-$(id -u)'</span>
 enroot_config: |
 ENROOT_CONFIG_PATH <span class="k">${</span><span class="nv">HOME</span><span class="k">}</span>/.config/enroot
 ENROOT_SQUASH_OPTIONS <span class="nt">-noI</span> <span class="nt">-noD</span> <span class="nt">-noF</span> <span class="nt">-noX</span> <span class="nt">-no-duplicates</span>
 ENROOT_MOUNT_HOME y
 ENROOT_RESTRICT_DEV y
 ENROOT_ROOTFS_WRITABLE y
 ENROOT_RUNTIME_PATH <span class="o">{{</span> enroot_runtime_path <span class="o">}}</span>
 ENROOT_CACHE_PATH <span class="o">{{</span> enroot_cache_path <span class="o">}}</span>
 ENROOT_DATA_PATH <span class="o">{{</span> enroot_data_path <span class="o">}}</span>
 enroot_environ_config_files: <span class="o">[]</span>
 enroot_environ_config_files_dgx:
 - filename: 50-mellanox.env
     content: |
     <span class="nv">MELLANOX_VISIBLE_DEVICES</span><span class="o">=</span>all
 - filename: 50-mpi.env
     content: |
     <span class="nv">OMPI_MCA_btl_tcp_if_exclude</span><span class="o">=</span>lo,docker0,ib0,ib1,ib2,ib3
	
 <span class="c"># Singularity configuration</span>
 <span class="c"># Playbook: singularity, slurm-cluster</span>
 <span class="c"># Singularity target version</span>
 singularity_version: <span class="s2">"3.7.3"</span>
 singularity_conf_path: <span class="s2">"/etc/singularity/singularity.conf"</span>
 bind_paths: <span class="o">[]</span>
 <span class="c"># example:</span>
 <span class="c">#- /mnt/shared</span>
 golang_install_dir: <span class="s1">'/opt/go/{{ golang_version }}'</span>
 golang_gopath: /opt/go/packages
	
 <span class="c">################################################################################</span>
 <span class="c"># AUTH                                                                         #</span>
 <span class="c">################################################################################</span>
 <span class="c"># NIS configuration</span>
 <span class="c"># Playbook: authentication</span>
 <span class="c">#nis_domain: example.com</span>
 <span class="c">#nis_server: 10.0.0.1</span>
	
 <span class="c"># Kerberos configuration</span>
 <span class="c"># Playbook: authentication</span>
 <span class="c">#kerberos_client_realm_name: "example.com"</span>
 <span class="c">#kerberos_client_kdc_hostname: "kerberos"</span>
 <span class="c">#kerberos_client_admin_hostname: "kerberos"</span>
 <span class="c">#nfs_idmapd_domain: example.com</span>
	
	
 <span class="c">################################################################################</span>
 <span class="c"># MONITORING                                                                   #</span>
 <span class="c">################################################################################</span>
 <span class="c"># Collectd</span>
 <span class="c"># Playbook: collectd</span>
 <span class="c">#collectd_network_server: "deepops-mgmt.example.com"</span>
 <span class="c">#collectd_network_port: "30300"</span>
 <span class="c">#collectd_python_module_path: "/usr/lib/collectd/dcgm"</span>
 <span class="c">#collectd_python_modules: []</span>
 <span class="c">#collectd_config_dir: "/etc/collectd/collectd.conf.d"</span>
 <span class="c">#with_dcgm_collectd: false</span>
	
	
 <span class="c">################################################################################</span>
 <span class="c"># MAAS                                                                         #</span>
 <span class="c">################################################################################</span>
 <span class="c"># MAAS (Metal as a Service): Node provisioning/PXE service</span>
 <span class="c"># Playbook: maas, maas_management</span>
 maas_adminusers:
 - username: <span class="s1">'admin'</span>
     email: <span class="s1">'admin@{{ maas_dns_domain }}'</span>
     password: <span class="s1">'admin'</span>
 maas_dns_domain: <span class="s1">'deepops.local'</span>
 maas_region_controller: <span class="s1">'192.168.1.1'</span>
 maas_region_controller_url: <span class="s1">'http://{{ maas_region_controller }}:5240/MAAS'</span>
 maas_repo: <span class="s1">'ppa:maas/2.8'</span>
	
 <span class="c"># Defines if maas user should generate ssh keys</span>
 <span class="c"># Usable for remote KVM/libvirt power actions</span>
 maas_setup_user: <span class="nb">false
	
 </span>maas_single_node_install: <span class="nb">true
	
 </span>maas_kvm: <span class="nb">false</span>
	
 <span class="c">################################################################################</span>
 <span class="c"># NVIDIA Datacenter GPU Manager                                                #</span>
 <span class="c">################################################################################</span>
 install_dcgm: <span class="nb">true</span>
	
 <span class="c">################################################################################</span>
 <span class="c"># Misc.                                                                        #</span>
 <span class="c">################################################################################</span>
 <span class="c"># Set /etc/rc.local contents</span>
 <span class="c"># Playbook: rc-local</span>
 <span class="c"># rc_local_contents: |</span>
 <span class="c">#   echo foo</span>
 <span class="c">#   echo bar</span>
 <span class="c">#</span>
 <span class="c"># DeepOps specific config</span>
 deepops_dir: /opt/deepops
 <span class="c"># Directory for python virtualenv</span>
 <span class="c"># Roles: K8s GPU operator, GPU plugin, OpenShift/K8s</span>
 deepops_venv: <span class="s1">'{{ deepops_dir }}/venv'</span>
	
 <span class="c"># OpenMPI</span>
 <span class="c"># Playbook: openmpi</span>
 openmpi_version: 4.0.3
	
 <span class="c"># Disable cloud-init</span>
 <span class="c"># ☆ 다음을 변경</span>
 deepops_disable_cloud_init: <span class="nb">false</span>
	
 <span class="c"># Default profile when using NVIDIA MIG Manager: https://github.com/NVIDIA/mig-parted</span>
 mig_manager_profile: all-disabled
 mig_manager_config: /etc/nvidia-mig-manager/config.yml
 mig_manager_hooks: /etc/nvidia-mig-manager/hooks.yaml
	
 <span class="c">################################################################################</span>
 <span class="c"># Container registry                                                           #</span>
 <span class="c">################################################################################</span>
 <span class="c"># ☆ 다음을 변경</span>
 standalone_container_registry_cache_enable: <span class="nb">false
 </span>standalone_container_registry_port: <span class="s2">"5000"</span>
	
 <span class="c"># To configure some set of hosts as insecure registries, list them here.</span>
 <span class="c"># Slurm and K8s cluster playbooks will automatically use the master nodes for</span>
 <span class="c"># these if not specified.</span>
 <span class="c">#docker_insecure_registries: ["&lt;host&gt;:&lt;port&gt;"]</span>
 <span class="c">#docker_registry_mirrors: ["http://&lt;host&gt;:&lt;port&gt;"]</span>
	
 <span class="c">################################################################################</span>
 <span class="c"># Configuration for NGC-Ready playbook                                         #</span>
 <span class="c">################################################################################</span>
 ngc_ready_cuda_container: <span class="s2">"nvcr.io/nvidia/cuda:10.1-base-ubuntu18.04"</span>
 ngc_ready_pytorch: <span class="s2">"nvcr.io/nvidia/pytorch:18.10-py3"</span>
 ngc_ready_tensorflow: <span class="s2">"nvcr.io/nvidia/tensorflow:18.10-py3"</span>
</pre></table></code></div></div><li>provisioning 노드에서 deepops slurm-cluster 설정 정보를 수정한다.<div lang="console" class="language-console highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre><td class="rouge-code"><pre><span class="gp"> $</span><span class="w"> </span>vi ./config/group_vars/slurm-cluster.yml
</pre></table></code></div></div><div lang="shell" class="language-shell highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60
61
62
63
64
65
66
67
68
69
70
71
72
73
74
75
76
77
78
79
80
81
82
83
84
85
86
87
88
89
90
91
92
93
94
95
96
97
98
99
100
101
102
103
104
105
106
107
108
109
110
111
112
113
114
115
116
117
118
119
120
121
122
123
124
125
126
127
128
129
130
131
132
133
134
135
136
137
138
139
140
141
142
143
144
145
146
147
148
149
150
151
152
153
154
155
156
157
158
159
160
161
162
163
164
165
166
167
168
169
170
171
172
173
174
175
176
177
178
179
180
181
182
183
184
185
186
187
188
189
190
191
192
193
194
195
196
197
198
199
200
201
202
203
204
205
206
207
208
209
210
211
212
213
214
215
216
217
218
219
220
221
222
223
224
225
226
227
228
229
230
231
232
</pre><td class="rouge-code"><pre> <span class="c">################################################################################</span>
 <span class="c"># Slurm                                                                        #</span>
 <span class="c">################################################################################</span>
 <span class="c"># Slurm job scheduler configuration</span>
 <span class="c"># Playbook: slurm, slurm-cluster, slurm-perf, slurm-perf-cluster, slurm-validation</span>
 slurm_version: <span class="s2">"21.08.8-2"</span>
 slurm_install_prefix: /usr/local
 pmix_install_prefix: /opt/deepops/pmix
 hwloc_install_prefix: /opt/deepops/hwloc
 <span class="c">#slurm_user_home: /local/slurm</span>
 slurm_manage_gpus: <span class="nb">true</span>
 <span class="c">#slurm_cluster_name: deepops</span>
 <span class="c">#slurm_username: slurm</span>
 slurm_password: ReplaceWithASecurePasswordInTheVault
 <span class="c">#slurm_db_username: slurm</span>
 slurm_db_password: AlsoReplaceWithASecurePasswordInTheVault
 <span class="c">#slurm_max_job_timelimit: INFINITE</span>
 <span class="c">#slurm_default_job_timelimit:</span>
	
 <span class="c"># Ensure hosts file generation only runs across slurm cluster</span>
 hosts_add_ansible_managed_hosts_groups: <span class="o">[</span><span class="s2">"slurm-cluster"</span><span class="o">]</span>
	
 <span class="c"># Enable Slurm high-availability mode</span>
 <span class="c"># NOTE: The location for Slurm saved state needs to reside in a location shared by all Slurm controller nodes</span>
 <span class="c">#       Ideally this is on external NFS server and not the primary control node, since its failure</span>
 <span class="c">#       would also take down that NFS share</span>
 slurm_enable_ha: <span class="nb">false
 </span>slurm_ha_state_save_location: <span class="s2">"/sw/slurm"</span>
	
 <span class="c"># Slurm configuration is auto-generated from templates in the `slurm` role.</span>
 <span class="c"># If you want to override any of these files with custom config files, please</span>
 <span class="c"># set the following vars to the absolute path of your custom files.</span>
 <span class="c">#</span>
 <span class="c">#slurm_conf_template: "/path/to/slurm.conf"</span>
 <span class="c">#slurm_cgroup_conf_template: "/path/to/cgroup.conf"</span>
 <span class="c">#slurm_gres_conf_template: "/path/to/gres.conf"</span>
 <span class="c">#slurm_dbd_conf_template: "/path/to/slurmdbd.conf"</span>
	
 <span class="c">################################################################################</span>
 <span class="c"># Login on compute</span>
 <span class="c">################################################################################</span>
 <span class="c"># Restrict user SSH access to only allow users with a running job</span>
 slurm_restrict_node_access: <span class="nb">true</span>
	
 <span class="c"># List users who should always be able to SSH, even without a running job</span>
 slurm_allow_ssh_user: <span class="o">[]</span>
	
 <span class="c"># Enable a Slurm compute node to also function as a login node.</span>
 <span class="c"># This is needed for the "single node Slurm cluster" configuration.</span>
 <span class="c"># See docs/slurm-cluster/slurm-single-node.md</span>
 slurm_login_on_compute: <span class="nb">false</span>
	
	
 <span class="c">################################################################################</span>
 <span class="c"># Optional installs                                                            #</span>
 <span class="c">################################################################################</span>
 slurm_configure_etc_hosts: <span class="nb">yes
 </span>dns_mode: none
 <span class="c"># ☆ 다음을 변경</span>
 slurm_cluster_install_cuda: no
 <span class="c"># ☆ 다음을 변경</span>
 slurm_cluster_install_nvidia_driver: no
 slurm_cluster_install_singularity: no
	
 <span class="c">################################################################################</span>
 <span class="c"># NFS                                                                          #</span>
 <span class="c">################################################################################</span>
 <span class="c"># Default exports:</span>
 <span class="c"># - /home: shared home directories, needed for Open OnDemand and best practice</span>
 <span class="c"># - /sw:   shared space for software installs, used for Spack or EasyBuild</span>
 nfs_exports:
 - path: /home
     options: <span class="s2">"*(rw,sync,no_root_squash)"</span>
 - path: /sw
     options: <span class="s2">"*(rw,sync,no_root_squash)"</span>
 nfs_mounts:
 - mountpoint: /home
     server: <span class="s1">'{{ groups["slurm-master"][0] }}'</span>
     path: /home
     options: async,vers<span class="o">=</span>3
 - mountpoint: /sw
     server: <span class="s1">'{{ groups["slurm-master"][0] }}'</span>
     path: /sw
     options: async,vers<span class="o">=</span>3
	
 <span class="c"># Flags for enable/disable of NFS deployment</span>
 <span class="c">#  - Set up an NFS server on nfs-server?</span>
 <span class="c"># ☆ 다음을 변경</span>
 slurm_enable_nfs_server: <span class="nb">false</span>
 <span class="c">#  - Mount NFS filesystems on nfs-clients?</span>
 <span class="c"># ☆ 다음을 변경</span>
 slurm_enable_nfs_client_nodes: <span class="nb">false</span>
	
 <span class="c"># Inventory host groups to use for NFS server or clients</span>
 nfs_server_group: <span class="s2">"slurm-master[0]"</span>
 nfs_client_group: <span class="s2">"slurm-master[1:],slurm-node"</span>
	
 <span class="c">################################################################################</span>
 <span class="c"># SOFTWARE MODULES (SM)                                                        #</span>
 <span class="c">#   May be built with either EasyBuild or Spack                                #</span>
 <span class="c">################################################################################</span>
 <span class="c"># ☆ 다음을 변경</span>
 slurm_install_lmod: <span class="nb">false</span>
	
 <span class="c"># Note: the sm_prefix must be in an NFS-shared location</span>
 sm_prefix: <span class="s2">"/sw"</span>
	
 <span class="c"># Easybuild-specific</span>
 sm_module_root: <span class="s2">"{{ sm_prefix }}/modules"</span>
 sm_software_path: <span class="s2">"{{ sm_prefix }}/software"</span>
 sm_files_path: <span class="s2">"{{ sm_prefix }}/easybuild_files"</span>
 sm_sources_path: <span class="s2">"{{ sm_prefix }}/sources"</span>
 sm_build_path: <span class="s2">"{{ sm_prefix }}/build"</span>
 sm_files_url: <span class="s2">"https://github.com/DeepOps/easybuild_files.git"</span>
 sm_install_default: <span class="nb">true
 </span>sm_module_path: <span class="s2">"{{ sm_module_root }}/all"</span>
	
 <span class="c"># Spack-specific</span>
 spack_install_dir: <span class="s2">"{{ sm_prefix }}/spack"</span>
 spack_build_packages: <span class="nb">false
 </span>spack_default_packages:
 - <span class="s2">"cuda@10.2.89"</span>
 - <span class="s2">"openmpi@3.1.6 +cuda +pmi schedulers=auto"</span>
	
 <span class="c"># Which host should we run installs on for software going into the NFS share?</span>
 sm_install_host: <span class="s2">"slurm-master[0]"</span>
	
 <span class="c">################################################################################</span>
 <span class="c"># NVIDIA HPC SDK                                                               #</span>
 <span class="c">################################################################################</span>
 <span class="c"># ☆ 다음을 변경</span>
 slurm_install_hpcsdk: <span class="nb">false</span>
	
 <span class="c"># Select the version of HPC SDK to download</span>
 hpcsdk_major_version: <span class="s2">"22"</span>
 hpcsdk_minor_version: <span class="s2">"1"</span>
 hpcsdk_file_cuda: <span class="s2">"11.5"</span>
 hpcsdk_arch: <span class="s2">"x86_64"</span>
	
 <span class="c"># In a Slurm cluster, default to setting up HPC SDK as modules rather than in</span>
 <span class="c"># the default user environment</span>
 <span class="c"># ☆ 다음을 변경</span>
 hpcsdk_install_as_modules: <span class="nb">false
 </span>hpcsdk_install_in_path: <span class="nb">false</span>
	
 <span class="c">################################################################################</span>
 <span class="c"># OpenMPI build                                                                #</span>
 <span class="c">#                                                                              #</span>
 <span class="c"># The openmpi.yml playbook will build a custom-configured OpenMPI based on the #</span>
 <span class="c"># Slurm, PMIx, and hwloc on the cluster. In most cases you should be fine with #</span>
 <span class="c"># using the OpenMPI provided in the HPC SDK, but if you need a custom build,   #</span>
 <span class="c"># this can help you get started.</span>
 <span class="c">################################################################################</span>
 slurm_cluster_install_openmpi: <span class="nb">false
 </span>openmpi_version: 4.0.4
 openmpi_install_prefix: <span class="s2">"/usr/local"</span>
 openmpi_configure: <span class="s2">"./configure --prefix={{ openmpi_install_prefix }} --disable-dependency-tracking --disable-getpwuid --with-pmix={{ pmix_install_prefix }} --with-hwloc={{ hwloc_install_prefix }} --with-pmi={{ slurm_install_prefix }} --with-slurm={{ slurm_install_prefix }} --with-libevent=/usr"</span>
	
 <span class="c">################################################################################</span>
 <span class="c"># Open OnDemand                                                                #</span>
 <span class="c">################################################################################</span>
 install_open_ondemand: no
 <span class="c"># OOD Linux-host adapter requires `slurm_cluster_install_singularity` to be true</span>
 ood_install_linuxhost_adapter: no
	
 servername: <span class="s1">'{{ ansible_fqdn }}'</span>
 httpd_port: 9050
 httpd_listen_addr_port:
 - 9050
 httpd_use_rewrites: <span class="nb">false
 </span>node_uri: /node
 rnode_uri: /rnode
	
 <span class="c">################################################################################</span>
 <span class="c"># Allow the User Permission to Set GPU Clocks                                  #</span>
 <span class="c">################################################################################</span>
 allow_user_set_gpu_clocks: no
	
 <span class="c">################################################################################</span>
 <span class="c"># Enroot &amp; Pyxis                                                               #</span>
 <span class="c">################################################################################</span>
 <span class="c"># ☆ 다음을 변경</span>
 slurm_install_enroot: <span class="nb">false</span>
 <span class="c"># ☆ 다음을 변경</span>
 slurm_install_pyxis: <span class="nb">false
 </span>slurm_pyxis_version: 0.11.1
	
 <span class="c">################################################################################</span>
 <span class="c"># Node Health Check                                                            #</span>
 <span class="c">################################################################################</span>
 <span class="c"># ☆ 다음을 변경</span>
 slurm_install_nhc: no
 slurm_health_check_program: <span class="s2">"/usr/sbin/nhc"</span>
	
 <span class="c"># The health check configuration generated by default in DeepOps is pretty</span>
 <span class="c"># basic, and most cluster administrators will want to set up more extensive</span>
 <span class="c"># NHC configurations with their local site customizations.</span>
 <span class="c"># To set a custom file for Ansible to provision to /etc/nhc/nhc.conf, set the</span>
 <span class="c"># following var:</span>
 <span class="c">#</span>
 <span class="c">#nhc_config_template: "/path/to/custom/nhc.conf"</span>
 <span class="c">#</span>
 <span class="c"># For documentation on the file format, see:</span>
 <span class="c"># https://github.com/mej/nhc/blob/master/README.md</span>
	
 <span class="c">################################################################################</span>
 <span class="c"># Container registry                                                           #</span>
 <span class="c">################################################################################</span>
 <span class="c"># ☆ 다음을 변경</span>
 slurm_enable_container_registry: <span class="nb">false
 </span>docker_insecure_registries: <span class="s2">"{{ groups['slurm-master'] | map('regex_replace', '^(.*)</span><span class="nv">$'</span><span class="s2">, '</span><span class="se">\\</span><span class="s2">1:5000') | list + ['registry.local:31500'] }}"</span>
 docker_registry_mirrors: <span class="s2">"{{ groups['slurm-master'] | map('regex_replace', '^(.*)</span><span class="nv">$'</span><span class="s2">, 'http://</span><span class="se">\\</span><span class="s2">1:5000') | list }}"</span>
	
 <span class="c">################################################################################</span>
 <span class="c"># Monitoring stack                                                             #</span>
 <span class="c">################################################################################</span>
 slurm_enable_monitoring: <span class="nb">true</span>
	
 <span class="c"># Inventory host groups where cluster monitoring services will be installed</span>
 <span class="c"># (Prometheus, Grafana, etc)</span>
 slurm_monitoring_group: <span class="s2">"slurm-master"</span>
	
 <span class="c">################################################################################</span>
 <span class="c"># Logging with rsyslog                                                         #</span>
 <span class="c">################################################################################</span>
 <span class="c"># ☆ 다음을 변경</span>
 slurm_enable_rsyslog_server: <span class="nb">false</span>
 <span class="c"># ☆ 다음을 변경</span>
 slurm_enable_rsyslog_client: <span class="nb">false
 </span>rsyslog_server_hostname: <span class="s2">"{{ groups['slurm-master'][0] }}"</span>
 rsyslog_client_tcp_host: <span class="s2">"{{ rsyslog_server_hostname }}"</span>
 rsyslog_client_group: <span class="s2">"slurm-cluster"</span>
</pre></table></code></div></div><li>provisioning 노드에서 deepops slurm-cluster 설치 정보를 수정한다.<div lang="console" class="language-console highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre><td class="rouge-code"><pre><span class="gp"> $</span><span class="w"> </span>vi ./playbooks/slurm-cluster.yml
</pre></table></code></div></div><div lang="shell" class="language-shell highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60
61
62
63
64
65
66
67
68
69
70
71
72
73
74
75
76
77
78
79
80
81
82
83
84
85
86
87
88
89
90
91
92
93
94
95
96
97
98
99
100
101
102
103
104
105
106
107
108
109
110
111
112
113
114
115
116
117
118
119
120
121
122
123
124
125
126
127
128
129
130
131
132
133
134
135
136
137
138
139
140
141
142
143
144
145
146
</pre><td class="rouge-code"><pre> <span class="c"># ☆ 다음을 변경 (모두 주석 처리 후, 3개 부분만 주석 해제)</span>
 <span class="nt">---</span>
 <span class="c"># Slurm Cluster Playbook</span>
	
 <span class="c"># Install python required for Ansible</span>
 <span class="c">#- include: bootstrap/bootstrap-python.yml</span>
	
 <span class="c"># Set up passwordless sudo and SSH keys if needed</span>
 <span class="c">#- include: bootstrap/bootstrap-ssh.yml</span>
 <span class="c">#- include: bootstrap/bootstrap-sudo.yml</span>
	
 <span class="c"># Disable cloud-init</span>
 <span class="c">#- include: generic/disable-cloud-init.yml</span>
 <span class="c">#  when: deepops_disable_cloud_init</span>
	
 <span class="c"># Configure hostnames, /etc/hosts</span>
 <span class="c"># ☆ 다음을 주석 처리 해제</span>
 - include: generic/hosts.yml
 when: <span class="s2">"{{ slurm_configure_etc_hosts | default(true) }}"</span>
 tags:
 - set-etc-hosts
	
 <span class="c"># Configure Chrony (NTP) sync</span>
 <span class="c">#- include: generic/chrony-client.yml</span>
 <span class="c">#  when: chrony_install|default(true)</span>
	
 <span class="c"># Set up a local cluster container registry</span>
 <span class="c">#- include: container/standalone-container-registry.yml hostlist=slurm-master</span>
 <span class="c">#  when: slurm_enable_container_registry|default(true)</span>
	
 <span class="c"># Set up NGINX-based container caching</span>
 <span class="c">#- include: container/nginx-docker-registry-cache-server.yml</span>
 <span class="c">#  vars:</span>
 <span class="c">#    hostlist: "{{ nginx_docker_cache_hostgroup | default('slurm-cache') }}"</span>
 <span class="c">#  when: slurm_enable_nginx_docker_cache | default(false)</span>
 <span class="c">#- include: container/nginx-docker-registry-cache-client.yml</span>
 <span class="c">#  vars:</span>
 <span class="c">#    hostlist: "{{ nginx_docker_cache_clients | default('slurm-node') }}"</span>
 <span class="c">#  when: slurm_enable_nginx_docker_cache | default(false)</span>
	
 <span class="c"># Install NVIDIA driver</span>
 <span class="c">#- include: nvidia-software/nvidia-driver.yml</span>
 <span class="c">#  when: slurm_cluster_install_nvidia_driver|default(true)</span>
	
 <span class="c"># Install NVIDIA CUDA Toolkit</span>
 <span class="c">#   Note: the CUDA playbook also installs the driver, so we pass the</span>
 <span class="c">#   appropriate flag to this playbook as well.</span>
 <span class="c">#- include: nvidia-software/nvidia-cuda.yml</span>
 <span class="c">#  vars:</span>
 <span class="c">#     cuda_playbook_install_driver: "{{ slurm_cluster_install_nvidia_driver }}"</span>
 <span class="c">#  when: slurm_cluster_install_cuda|default(true)</span>
	
 <span class="c"># Install software</span>
 <span class="c">#- include: generic/software.yml</span>
	
 <span class="c"># Set up NFS filesystem</span>
 <span class="c">#- include: generic/nfs-server.yml</span>
 <span class="c">#  vars:</span>
 <span class="c">#    hostlist: "{{ nfs_server_group | default('slurm-nfs[0]') }}"</span>
 <span class="c">#  when: slurm_enable_nfs_server</span>
 <span class="c">#- include: generic/nfs-client.yml</span>
 <span class="c">#  vars:</span>
 <span class="c">#    hostlist: "{{ nfs_client_group | default('slurm-nfs-client') }}"</span>
 <span class="c">#  when: slurm_enable_nfs_client_nodes</span>
	
 <span class="c"># Install DCGM</span>
 <span class="c">#- include: nvidia-software/nvidia-dcgm.yml hostlist=slurm-node</span>
 <span class="c">#  when: install_dcgm|default(false)</span>
	
 <span class="c"># Install Node Health Check</span>
 <span class="c">#- include: slurm-cluster/nhc.yml hostlist=slurm-node</span>
 <span class="c">#  when: slurm_install_nhc|default(false)</span>
	
 <span class="c"># Install Slurm</span>
 <span class="c">#- include: slurm-cluster/slurm.yml</span>
	
 <span class="c"># Install OpenMPI</span>
 <span class="c">#- include: slurm-cluster/openmpi.yml</span>
 <span class="c">#  when: slurm_cluster_install_openmpi|default(true)</span>
	
 <span class="c"># Install Lmod</span>
 <span class="c">#- include: slurm-cluster/lmod.yml</span>
 <span class="c">#  when: slurm_install_lmod</span>
	
 <span class="c"># Install the NVIDIA HPC SDK</span>
 <span class="c">#- include: nvidia-software/nvidia-hpc-sdk.yml</span>
 <span class="c">#  vars:</span>
 <span class="c">#    hostlist: "{{ sm_install_host | default('slurm-login[0]')}}"</span>
 <span class="c">#  when: slurm_install_hpcsdk</span>
	
 <span class="c"># Install monitoring services</span>
 <span class="c"># ☆ 다음을 주석 처리 해제</span>
 - include: slurm-cluster/prometheus.yml
 vars:
     hostlist: <span class="s2">"{{ slurm_monitoring_group | default('slurm-metric') }}"</span>
 when: slurm_enable_monitoring
 - include: slurm-cluster/grafana.yml
 vars:
     hostlist: <span class="s2">"{{ slurm_monitoring_group | default('slurm-metric') }}"</span>
 when: slurm_enable_monitoring
	
 <span class="c"># Install monitoring exporters</span>
 <span class="c">#- include: slurm-cluster/prometheus-slurm-exporter.yml</span>
 <span class="c">#  vars:</span>
 <span class="c">#    hostlist: "{{ slurm_monitoring_group | default('slurm-metric') }}"</span>
 <span class="c">#  when: slurm_enable_monitoring</span>
 <span class="c"># ☆ 다음을 주석 처리 해제</span>
 - include: slurm-cluster/prometheus-node-exporter.yml
 when: slurm_enable_monitoring
 - include: slurm-cluster/nvidia-dcgm-exporter.yml
 when: slurm_enable_monitoring
	
 <span class="c"># Set up rsyslog forwarding from compute nodes to head node</span>
 <span class="c">#- include: generic/rsyslog-server.yml</span>
 <span class="c">#  vars:</span>
 <span class="c">#    hostlist: "{{ rsyslog_server_hostname | default('slurm-master[0]') }}"</span>
 <span class="c">#  when: slurm_enable_rsyslog_server|default(true)</span>
 <span class="c">#- include: generic/rsyslog-client.yml</span>
 <span class="c">#  vars:</span>
 <span class="c">#    hostlist: "{{ rsyslog_client_group | default('slurm-node') }}"</span>
 <span class="c">#  when: slurm_enable_rsyslog_client|default(true)</span>
	
 <span class="c"># Install Singularity</span>
 <span class="c">#- include: container/singularity.yml</span>
 <span class="c">#  when: slurm_cluster_install_singularity|default(true)</span>
	
 <span class="c"># Install Open OnDemand</span>
 <span class="c">#- include: slurm-cluster/open-ondemand.yml</span>
 <span class="c">#  when: install_open_ondemand</span>
	
 <span class="c"># Set Permissions to adjust GPU Clocks speeds</span>
 <span class="c">#- include: utilities/gpu-clocks.yml</span>
 <span class="c">#  when: allow_user_set_gpu_clocks</span>
	
 <span class="c"># Install Enroot and Pyxis</span>
 <span class="c">#- include: container/pyxis.yml</span>
 <span class="c">#  when:</span>
 <span class="c">#    - slurm_install_enroot</span>
 <span class="c">#    - slurm_install_pyxis</span>
 <span class="c">#  tags:</span>
 <span class="c">#    - pyxis</span>
	
 <span class="c"># Ensure that nv_peer_mem is loaded</span>
 <span class="c">#- include: nvidia-software/nvidia-peer-memory.yml</span>
 <span class="c">#  tags:</span>
 <span class="c">#    - nvidia-peer-memory</span>
</pre></table></code></div></div><li>(Optional: nvidia driver, docker 설치는 되어 있다고 가정하는 경우 ★) provisioning 노드에서 deepops nvidia-dcgm-exporter 설치 정보를 수정한다.<div lang="console" class="language-console highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre><td class="rouge-code"><pre><span class="gp"> $</span><span class="w"> </span>vi ./playbooks/slurm-cluster/nvidia-dcgm-exporter.yml
</pre></table></code></div></div><div lang="shell" class="language-shell highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
</pre><td class="rouge-code"><pre> <span class="nt">---</span>
 <span class="c"># ☆ 다음을 주석 처리</span>
 <span class="c">#- include: ../container/docker.yml</span>
 <span class="c">#- include: ../nvidia-software/nvidia-driver.yml</span>
 <span class="c">#- include: ../container/nvidia-docker.yml</span>
	
 - hosts:  <span class="s2">"{{ hostlist | default('all') }}"</span>
 become: <span class="nb">yes
 </span>tasks:
     - name: <span class="nb">install </span>custom facts module
     include_role:
         name: facts
     - name: <span class="nb">set </span>GPU fact
     set_fact:
         has_gpus: <span class="nb">true
     </span>when: ansible_local[<span class="s1">'gpus'</span><span class="o">][</span><span class="s1">'count'</span><span class="o">]</span>
     - name: configure dcgm exporter
     include_role:
         name: nvidia-dcgm-exporter
     when: ansible_distribution <span class="o">==</span> <span class="s2">"Ubuntu"</span> or ansible_os_family <span class="o">==</span> <span class="s2">"RedHat"</span>
</pre></table></code></div></div><li>Slurm Cluster를 설치한다.<div lang="console" class="language-console highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
</pre><td class="rouge-code"><pre><span class="gp">#</span><span class="w"> </span>NOTE: If SSH requires a password, add: <span class="sb">`</span><span class="nt">-k</span><span class="sb">`</span>
<span class="gp">#</span><span class="w"> </span>NOTE: If <span class="nb">sudo </span>on remote machine requires a password, add: <span class="sb">`</span><span class="nt">-K</span><span class="sb">`</span>
<span class="gp">#</span><span class="w"> </span>NOTE: If SSH user is different than current user, add: <span class="sb">`</span><span class="nt">-u</span> ubuntu<span class="sb">`</span>
<span class="gp">$</span><span class="w"> </span>ansible-playbook <span class="nt">-kK</span> <span class="nt">-l</span> slurm-cluster playbooks/slurm-cluster.yml
</pre></table></code></div></div></ol><h2 id="접속">접속</h2><hr /><ol><li>master 노드의 node-exporter - 192.168.30.119:9100/metrics<li>master 노드의 dcgm-exporter - 192.168.30.119:9400/metrics<li>worker 노드의 node-exporter - 192.168.30.121:9100/metrics<li>worker 노드의 dcgm-exporter - 192.168.30.121:9400/metrics<li>master 노드의 prometheus - 192.168.30.119:9090/graph<li>master 노드의 grafana - 192.168.30.119:3000</ol><h2 id="참고">참고</h2><hr /><ol><li>Slurm Deployment Guide https://github.com/NVIDIA/deepops/tree/master/docs/slurm-cluster</ol></div><div class="post-tail-wrapper text-muted"><div class="post-meta mb-3"> <i class="far fa-folder-open fa-fw mr-1"></i> <a href='/categories/devops/'>DevOps</a>, <a href='/categories/ansible/'>Ansible</a></div><div class="post-tags"> <i class="fa fa-tags fa-fw mr-1"></i> <a href="/tags/devops/" class="post-tag no-text-decoration" >DevOps</a> <a href="/tags/ansible/" class="post-tag no-text-decoration" >Ansible</a></div><div class="post-tail-bottom d-flex justify-content-between align-items-center mt-3 pt-5 pb-2"><div class="license-wrapper"> This post is licensed under <a href="https://creativecommons.org/licenses/by/4.0/?v=1743601323"> CC BY 4.0 </a> by the author.</div><div class="share-wrapper"> <span class="share-label text-muted mr-1">Share</span> <span class="share-icons"> <a href="https://twitter.com/intent/tweet?text=nvidia/deeops 라이브러리를 이용한 모니터링(prometheus, grafana) 테스트 구성 - Develiberta&url=https://develiberta.github.io/posts/deepops_test/?v=1743601323" data-toggle="tooltip" data-placement="top" title="Twitter" target="_blank" rel="noopener" aria-label="Twitter"> <i class="fa-fw fab fa-twitter"></i> </a> <a href="https://www.facebook.com/sharer/sharer.php?title=nvidia/deeops 라이브러리를 이용한 모니터링(prometheus, grafana) 테스트 구성 - Develiberta&u=https://develiberta.github.io/posts/deepops_test/?v=1743601323" data-toggle="tooltip" data-placement="top" title="Facebook" target="_blank" rel="noopener" aria-label="Facebook"> <i class="fa-fw fab fa-facebook-square"></i> </a> <a href="https://telegram.me/share?text=nvidia/deeops 라이브러리를 이용한 모니터링(prometheus, grafana) 테스트 구성 - Develiberta&url=https://develiberta.github.io/posts/deepops_test/?v=1743601323" data-toggle="tooltip" data-placement="top" title="Telegram" target="_blank" rel="noopener" aria-label="Telegram"> <i class="fa-fw fab fa-telegram"></i> </a> <i class="fa-fw fas fa-link small" onclick="copyLink('', 'Link copied successfully!')" data-toggle="tooltip" data-placement="top" title="Copy link"> </i> </span></div></div></div></div></div><div id="panel-wrapper" class="col-xl-3 pl-2 text-muted topbar-down"><div class="access"><div id="access-lastmod" class="post"> <span>Recent Update</span><ul class="post-content pl-0 pb-1 ml-1 mt-2"><li><a href="/posts/os-definition/?v=1743601323">OS Definition</a><li><a href="/posts/os-storage-management/?v=1743601323">Storage Management</a><li><a href="/posts/os-virtual-memory/?v=1743601323">Virtual Memroy</a><li><a href="/posts/os-main-memory/?v=1743601323">Main Memroy</a><li><a href="/posts/os-deadlocks/?v=1743601323">Deadlocks</a></ul></div><div id="access-tags"> <span>Trending Tags</span><div class="d-flex flex-wrap mt-3 mb-1 mr-3"> <a class="post-tag" href="/tags/cs/?v=1743601323">CS</a> <a class="post-tag" href="/tags/os/?v=1743601323">OS</a> <a class="post-tag" href="/tags/securiy/?v=1743601323">Securiy</a> <a class="post-tag" href="/tags/ansible/?v=1743601323">Ansible</a> <a class="post-tag" href="/tags/devops/?v=1743601323">DevOps</a> <a class="post-tag" href="/tags/linux/?v=1743601323">Linux</a> <a class="post-tag" href="/tags/typography/?v=1743601323">typography</a> <a class="post-tag" href="/tags/algorithm/?v=1743601323">Algorithm</a> <a class="post-tag" href="/tags/favicon/?v=1743601323">favicon</a> <a class="post-tag" href="/tags/getting-started/?v=1743601323">getting started</a></div></div></div><script src="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.js"></script><div id="toc-wrapper" class="pl-0 pr-4 mb-5"> <span class="pl-3 pt-2 mb-2">Contents</span><nav id="toc" data-toggle="toc"></nav></div></div></div><div class="row"><div class="col-12 col-lg-11 col-xl-8"><div id="post-extend-wrapper" class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4"><div id="related-posts" class="mt-5 mb-2 mb-sm-4"><h3 class="pt-2 mt-1 mb-4 ml-1" data-toc-skip>Further Reading</h3><div class="card-deck mb-4"><div class="card"> <a href="/posts/deepops_basic/?v=1743601323"><div class="card-body"> <span class="timeago small" >Jan 20, 2022<i class="unloaded">2022-01-20T11:00:00+09:00</i> </span><h3 class="pt-0 mt-1 mb-3" data-toc-skip>nvidia/deeops 라이브러리를 이용한 모니터링(prometheus, grafana) 기본 구성</h3><div class="text-muted small"><p> 학습 목표 deepops가 무엇인지 이해하고 설명할 수 있다. deepops를 이용해서 기본적인 환경을 구성할 수 있다. deepops 개요 https://github.com/NVIDIA/deepops GPU 인프라 및 자동화 도구 deepops를 이용한 기본적인 환경 구성 Install a suppo...</p></div></div></a></div><div class="card"> <a href="/posts/os-protection/?v=1743601323"><div class="card-body"> <span class="timeago small" >Mar 14, 2023<i class="unloaded">2023-03-14T19:00:00+09:00</i> </span><h3 class="pt-0 mt-1 mb-3" data-toc-skip>Protection</h3><div class="text-muted small"><p> 목적 현대 컴퓨터 시스템에서 보호의 목적과 원칙에 대해 논의한다. 접근 행렬과 결합한 보호 도메인이 프로세스가 접근할 수 있는 자원을 지정하는 데 어떻게 사용되는지 설명한다. 자격- 및 언어- 기반의 보호 시스템을 설명한다. 보호 기법이 시스템 공격을 완화할 방법을 설명한다. 실천 목표 현대 컴퓨터 시스템에서 보호의 목적과...</p></div></div></a></div><div class="card"> <a href="/posts/os-security/?v=1743601323"><div class="card-body"> <span class="timeago small" >Mar 14, 2023<i class="unloaded">2023-03-14T18:30:00+09:00</i> </span><h3 class="pt-0 mt-1 mb-3" data-toc-skip>Security</h3><div class="text-muted small"><p> 목적 보안상의 문제점들(threats)과 공격에 대해 논의한다. 암호화, 인증, 해싱의 근본 원리를 설명한다. 연산에 있어서 암호 작성법의 사용을 검토한다. 보안상의 공격에 대한 다양한 대응책을 논의한다. 실천 목표 보안상의 문제점들(threats)과 공격에 대해 논의한다. 암호화, 인증, 해싱의 근본 원리를 설명한다....</p></div></div></a></div></div></div><div class="post-navigation d-flex justify-content-between"> <a href="/posts/system-basic-linux_filesystem/?v=1743601323" class="btn btn-outline-primary" prompt="Older"><p>유닉스/리눅스 기본 학습 - 파일시스템</p></a> <a href="/posts/os-getting-started/?v=1743601323" class="btn btn-outline-primary" prompt="Newer"><p>OS Getting Started</p></a></div><script src="https://utteranc.es/client.js" repo="develiberta/blog-comments" issue-term="url" theme="github-light" crossorigin="anonymous" async> </script> <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script></div></div></div><footer class="d-flex w-100 justify-content-center"><div class="d-flex justify-content-between align-items-center"><div class="footer-left"><p class="mb-0"> © 2025 <a href="https://twitter.com/develiberta">develiberta</a>. <span data-toggle="tooltip" data-placement="top" title="Except where otherwise noted, the blog posts on this site are licensed under the Creative Commons Attribution 4.0 International (CC BY 4.0) License by the author.">Some rights reserved.</span></p></div><div class="footer-right"><p class="mb-0"> Powered by <a href="https://jekyllrb.com" target="_blank" rel="noopener">Jekyll</a> with <a href="https://github.com/cotes2020/jekyll-theme-chirpy" target="_blank" rel="noopener">Chirpy</a> theme.</p></div></div></footer></div><div id="search-result-wrapper" class="d-flex justify-content-center unloaded"><div class="col-12 col-sm-11 post-content"><div id="search-hints"><h4 class="text-muted mb-4">Trending Tags</h4><a class="post-tag" href="/tags/cs/?v=1743601323">CS</a> <a class="post-tag" href="/tags/os/?v=1743601323">OS</a> <a class="post-tag" href="/tags/securiy/?v=1743601323">Securiy</a> <a class="post-tag" href="/tags/ansible/?v=1743601323">Ansible</a> <a class="post-tag" href="/tags/devops/?v=1743601323">DevOps</a> <a class="post-tag" href="/tags/linux/?v=1743601323">Linux</a> <a class="post-tag" href="/tags/typography/?v=1743601323">typography</a> <a class="post-tag" href="/tags/algorithm/?v=1743601323">Algorithm</a> <a class="post-tag" href="/tags/favicon/?v=1743601323">favicon</a> <a class="post-tag" href="/tags/getting-started/?v=1743601323">getting started</a></div><div id="search-results" class="d-flex flex-wrap justify-content-center text-muted mt-3"></div></div></div></div><div id="mask"></div><a id="back-to-top" href="#" aria-label="back-to-top" class="btn btn-lg btn-box-shadow" role="button"> <i class="fas fa-angle-up"></i> </a> <script src="https://cdn.jsdelivr.net/npm/simple-jekyll-search@1.7.3/dest/simple-jekyll-search.min.js"></script> <script> SimpleJekyllSearch({ searchInput: document.getElementById('search-input'), resultsContainer: document.getElementById('search-results'), json: '/assets/js/data/search.json', searchResultTemplate: '<div class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-lg-4 pr-lg-4 pl-xl-0 pr-xl-0"> <a href="https://develiberta.github.io{url}">{title}</a><div class="post-meta d-flex flex-column flex-sm-row text-muted mt-1 mb-1"> {categories} {tags}</div><p>{snippet}</p></div>', noResultsText: '<p class="mt-5">Oops! No result founds.</p>', templateMiddleware: function(prop, value, template) { if (prop === 'categories') { if (value === '') { return `${value}`; } else { return `<div class="mr-sm-4"><i class="far fa-folder fa-fw"></i>${value}</div>`; } } if (prop === 'tags') { if (value === '') { return `${value}`; } else { return `<div><i class="fa fa-tag fa-fw"></i>${value}</div>`; } } } }); </script> <script src="https://cdn.jsdelivr.net/combine/npm/lozad/dist/lozad.min.js,npm/magnific-popup@1/dist/jquery.magnific-popup.min.js"></script> <script defer src="/assets/js/dist/post.min.js?v=1743601323"></script> <script src="https://cdn.jsdelivr.net/combine/npm/popper.js@1.16.1,npm/bootstrap@4/dist/js/bootstrap.min.js"></script> <script defer src="/app.js?v=1743601323"></script> <script defer src="https://www.googletagmanager.com/gtag/js?id="></script> <script> document.addEventListener("DOMContentLoaded", function(event) { window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', ''); }); </script>
